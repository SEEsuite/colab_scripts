{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPDD+58RwdaHWkVeIL9dFaB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SEEsuite/colab_scripts/blob/dev/reddit_edgelist_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Oct 22, 2022\n",
        "\n",
        "Take reddit data and get \"edge list\" and a few other things\n",
        "\n",
        "Edge list is a graphs where nodes are users and edges are the interactions between them.\n"
      ],
      "metadata": {
        "id": "baA3ffzBgFSW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8aSQWjSf3s4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import datetime\n",
        "import time\n",
        "import math\n",
        "import json\n",
        "import requests\n",
        "import itertools\n",
        "import numpy as np\n",
        "import time\n",
        "from datetime import datetime, timedelta"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_time(my_time):\n",
        "    time_str = my_time\n",
        "    utc_time = datetime.datetime.strptime(time_str, '%Y-%m-%d %H:%M:%S.%f')\n",
        "    a = time.mktime(datetime.datetime.strptime(time_str,\n",
        "                                             '%Y-%m-%d %H:%M:%S.%f').timetuple())\n",
        "\n",
        "    print(a)\n",
        "    return int(a)\n",
        "    "
      ],
      "metadata": {
        "id": "LiF3L9RhgoEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "def import_data_from_drive(share_link, your_name_for_file=\"my_data\"):\n",
        "  \"\"\"Brings data file from a google drive sharepoint to your colab workspace.\n",
        "     It does not require you to host the dataset on your own account.\n",
        "\n",
        "     Parameters:\n",
        "     share_link: the link to view a file in google drive\n",
        "     our_name_for_file: a string describing the file, preferable endling in a file type, ex. 'data.csv'\n",
        "     \"\"\"\n",
        "  id = share_link.split(\"/\")[5] # separate the id from the link\n",
        "  print(\"Using id\", id, \"to find file on drive\")\n",
        "\n",
        "  # use pydrive and colab modules to authenticate you\n",
        "  auth.authenticate_user()\n",
        "  gauth = GoogleAuth()\n",
        "  gauth.credentials = GoogleCredentials.get_application_default()\n",
        "  drive = GoogleDrive(gauth)\n",
        "  print(\"Authenticated colab user\")\n",
        "\n",
        "  # This step will move the file from Drive to the workspace\n",
        "  download_object = drive.CreateFile({'id':id}) \n",
        "  download_object.GetContentFile(your_name_for_file)\n",
        "  print(\"Added file to workspace with name\", your_name_for_file)\n",
        "\n",
        "  return"
      ],
      "metadata": {
        "id": "r7XjLhh3guTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load data from folder of files\n",
        "If the data is scattered, pass in the path to the single folder containing data."
      ],
      "metadata": {
        "id": "4FZ2MZx8gKcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip /content/RedditData2022.zip"
      ],
      "metadata": {
        "id": "JJ2o93cbimj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CHANGE THIS\n",
        "directory = '/content/reddit_data'\n",
        "\n",
        "o_df = pd.DataFrame()\n",
        "print( os.listdir(directory))\n",
        "for f in os.listdir(directory):\n",
        "    if '$' in f:\n",
        "        continue\n",
        "    file = os.path.join(directory, f)\n",
        "    print(file)\n",
        "    sub_df = pd.read_excel(file, header=7)\n",
        "    assert 'Full Text' in sub_df.columns\n",
        "    o_df = pd.concat((o_df, sub_df), axis=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_E_BrJWgIbz",
        "outputId": "129258db-515f-49a3-c015-a972fcf11580"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['mentions (35).xlsx', 'mentions (34).xlsx', 'mentions (44).xlsx', 'mentions (40).xlsx', 'mentions (43).xlsx', 'mentions (42).xlsx', 'mentions (36).xlsx', 'mentions (38).xlsx', 'mentions (41).xlsx', 'mentions (39).xlsx', 'mentions (37).xlsx']\n",
            "/content/reddit_data/mentions (35).xlsx\n",
            "/content/reddit_data/mentions (34).xlsx\n",
            "/content/reddit_data/mentions (44).xlsx\n",
            "/content/reddit_data/mentions (40).xlsx\n",
            "/content/reddit_data/mentions (43).xlsx\n",
            "/content/reddit_data/mentions (42).xlsx\n",
            "/content/reddit_data/mentions (36).xlsx\n",
            "/content/reddit_data/mentions (38).xlsx\n",
            "/content/reddit_data/mentions (41).xlsx\n",
            "/content/reddit_data/mentions (39).xlsx\n",
            "/content/reddit_data/mentions (37).xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data from single excel sheet"
      ],
      "metadata": {
        "id": "gbubqLCAiAtP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "link = \"https://docs.google.com/spreadsheets/d/1m1-qV00Qkm2m9Znypj_ORBZgAQ9yQ9eO/edit?usp=sharing&ouid=101042095541764641159&rtpof=true&sd=true\"\n",
        "import_data_from_drive(link, your_name_for_file=\"tweets.xlsx\")\n",
        "df = pd.read_excel('tweets.xlsx')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTlb-iXaiBR8",
        "outputId": "21576794-6143-4c4f-8d80-ff7e0720e90d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/RedditData2022.zip\n",
            "warning:  stripped absolute path spec from /\n",
            "mapname:  conversion of  failed\n",
            " extracting: mentions (44).xlsx      \n",
            " extracting: mentions (35).xlsx      \n",
            " extracting: mentions (41).xlsx      \n",
            " extracting: mentions (43).xlsx      \n",
            " extracting: mentions (36).xlsx      \n",
            " extracting: mentions (37).xlsx      \n",
            " extracting: mentions (38).xlsx      \n",
            " extracting: mentions (39).xlsx      \n",
            " extracting: mentions (40).xlsx      \n",
            " extracting: mentions (42).xlsx      \n",
            " extracting: mentions (34).xlsx      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess"
      ],
      "metadata": {
        "id": "TYDsmUoniCBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def do(x):\n",
        "    if '2022-09-11' in x or '2022-08-31' in x:\n",
        "        return pd.NA\n",
        "    else:\n",
        "        return x"
      ],
      "metadata": {
        "id": "t0aPvRogisWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = o_df.copy()"
      ],
      "metadata": {
        "id": "l14XLGU4gIib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(df), \"instances\")\n",
        "df['Date'] = df['Date'].apply(do)\n",
        "df = df.dropna(subset=['Date'])\n",
        "print(len(df), \"instances\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MLq_nn9gInl",
        "outputId": "267a482d-f327-4240-bc98-b3e66679baa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34579 instances\n",
            "31277 instances\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove hard duplicates\n",
        "crop_df = df.drop_duplicates(subset=[\"Full Text\"], ignore_index=True).copy()\n",
        "len(crop_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6cmGxJRgIq-",
        "outputId": "87d594c2-454a-4b0c-f0e4-cd55b4b8b90c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28140"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this block is borrowed from tutorial\n",
        "def make_request(uri, max_retries = 2):\n",
        "    def fire_away(uri):\n",
        "        response = requests.get(uri)\n",
        "        print(response.status_code)\n",
        "        assert response.status_code == 200\n",
        "        return json.loads(response.content)\n",
        "    current_tries = 1\n",
        "    while current_tries < max_retries:\n",
        "        try:\n",
        "            time.sleep(1.1)\n",
        "            response = fire_away(uri)\n",
        "            return response\n",
        "        except:\n",
        "            time.sleep(1.1)\n",
        "            current_tries += 1\n",
        "    return fire_away(uri)"
      ],
      "metadata": {
        "id": "6wNG4vpigIt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def  get_ids(comment_id):\n",
        "    url = 'https://api.pushshift.io/reddit/comment/search?ids=' + comment_id\n",
        "#     print(url)\n",
        "    ret = {'parent_id': None, 'link_id': None, 'body': None, 'author':None, 'author_flair': None}\n",
        "    j = make_request(url)\n",
        "    j = j['data'][0]\n",
        "    ret['body'] = j['body']\n",
        "    ret['link_id'] = np.base_repr(int(j['link_id']), base=10)\n",
        "    ret['parent_id'] = j['parent_id']\n",
        "    ret['author'] = j['author']\n",
        "    ret['author_flair'] = j['author_flair_text']\n",
        "#     print(ret['parent_id'])\n",
        "    \n",
        "    return ret\n",
        "\n",
        "def get_poster(sub_id):\n",
        "#     parent_id = np.base_repr(int(sub_id), base=36)\n",
        "    new_url = 'https://api.pushshift.io/reddit/submission/search?ids=' +sub_id\n",
        "    print(\"NEW\", new_url)\n",
        "    j = make_request(new_url)\n",
        "    j = j['data'][0]\n",
        "    auth = j['author']\n",
        "    body = j['title']\n",
        "    flair = None\n",
        "    if 'author_flair_text' in j.keys():\n",
        "        flair = j['author_flair_text']\n",
        "    elif'author_flair_richtext' in j.keys():\n",
        "        flair = j['author_flair_richtext']\n",
        "    return auth, body, flair\n",
        "\n",
        "def get_parent_author(parent_id):\n",
        "    parent_id = np.base_repr(int(parent_id), base=10)\n",
        "    new_url = 'https://api.pushshift.io/reddit/comment/search?ids=' + parent_id\n",
        "    j = make_request(new_url)\n",
        "    j = j['data'][0]\n",
        "    auth = j['author']\n",
        "    body = j['body']\n",
        "    flair = None\n",
        "    if 'author_flair_text' in j.keys():\n",
        "        flair = j['author_flair_text']\n",
        "    return auth, body, flair\n",
        "\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "Vy-yMp5bgIw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def my_parse(x):\n",
        "#     print(x)\n",
        "    url = x\n",
        "    author = None\n",
        "    parent_author = None\n",
        "    comment_id = None\n",
        "    parent_id = None \n",
        "    link_id = None \n",
        "    interaction_type = 'comment'\n",
        "    author_flair = None\n",
        "    parent_author_flair = None\n",
        "    body= None\n",
        "    parent_body= None\n",
        "    failure = False\n",
        "    \n",
        "\n",
        "    parse = url.split('/')\n",
        "    parse = [p for p in parse if p != '']\n",
        "#     print(parse)\n",
        "    assert parse[4] == 'comments'\n",
        "    comment_id = parse[-1]\n",
        "    parse = parse[5:]\n",
        "\n",
        "    if len(parse) == 2:\n",
        "#         print(\"self\")\n",
        "        interaction_type = 'post'\n",
        "\n",
        "        \n",
        "#         author, body, flair = get_poster(comment_id)\n",
        "\n",
        "    elif len(parse) >2:\n",
        "        d = get_ids(comment_id)\n",
        "        body = d['body']\n",
        "        author = d['author']\n",
        "        author_flair = d['author_flair']\n",
        "        if not d['parent_id']:\n",
        "#             print(\"1st comment\")\n",
        "            link_id = d['link_id']\n",
        "            parent_author, parent_body, parent_flair = get_poster(link_id)\n",
        "            pass\n",
        "        else:\n",
        "#             print(\"nth comment\")\n",
        "            parent_author, parent_body, parent_author_flair = get_parent_author(d['parent_id'])\n",
        "#     except:\n",
        "#         print(\"Failed on\", url)\n",
        "#         failure = True\n",
        "    x = [author, parent_author, comment_id, parent_id, link_id, interaction_type, author_flair, parent_author_flair, body, parent_body, url]\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "c8dB9j3bgI0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "series = crop_df['Url']\n",
        "series.apply(my_parse)"
      ],
      "metadata": {
        "id": "j6tffe8ngI3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lkk8u90-gI6L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
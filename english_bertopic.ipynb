{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SEEsuite/colab_scripts/blob/main/english_bertopic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8M7OjN4mvT4x"
      },
      "source": [
        "\n",
        "Bertopic is a model designed by Maarten Grootendorst that comes with a pretty great python library. There are, for example, a good bit of plots that you can generate immediately. This script generates 3 files - the fitted model, the tweets with topic labels one-hot encoded, and the topics by their top keywords. The script is not meant to be comprehensive - Grootendorst provides a bunch of other example colabs.\n",
        "\n",
        "\n",
        "BERTopic will contextually embed tweets, then cluster them into topic groups. The number of topics is not neccesarily set, but you can manually fold topics togethr or adjust parts of the model. This script should work as is, but if the results are not great for you data, look into adjusting subcomponents of the model, like the hugging face embedding model, UMAP, or the clustering tool used. There is plenty of documentation [here](https://maartengr.github.io/BERTopic/index.html#:~:text=BERTopic%20is%20a%20topic%20modeling,words%20in%20the%20topic%20descriptions.). There are a million changes you could make to this model, so don't get too caught up in it - if it's not working, cut your losses.\n",
        "\n",
        "\n",
        "The text may benefit from cleaning, to match the training set of the embedding model. Definitely consider removing punctuation. However, the model will be pretty robust, more so than traditional topic modeling like LDR. Consider removing digits as well. \n",
        "\n",
        "\n",
        "[paper](https://arxiv.org/abs/2203.05794)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2HmkYvCpTbM"
      },
      "outputs": [],
      "source": [
        "### HERE IS THE CELL YOU NEED TO CHANGE\n",
        "link = \"https://docs.google.com/spreadsheets/d/1m1-qV00Qkm2m9Znypj_ORBZgAQ9yQ9eO/edit?usp=sharing&ouid=101042095541764641159&rtpof=true&sd=true\"\n",
        "model_name = \"basic_english_model\"\n",
        "### Expects text to be in a df called 'Full Text'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjbYyANcm56d"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install bertopic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsSdgJngKR2i"
      },
      "outputs": [],
      "source": [
        "# huggingface's tools for pretrained language models\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zY5AZTrLyT3f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import tqdm\n",
        "import pandas as pd\n",
        "from nltk import TweetTokenizer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from bertopic import BERTopic\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import hdbscan\n",
        "from sklearn.cluster import KMeans\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import string\n",
        "import re # search through and clean text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PYEMqpAo6De"
      },
      "outputs": [],
      "source": [
        "# importing miscelaneaous packages \n",
        "import numpy as np # fast manipulation of multidimensional arrays\n",
        "\n",
        "from tqdm.notebook import tqdm as progress_bar # a little vizualization of how fast a loop is running\n",
        "from scipy.special import softmax\n",
        "import pandas as pd # basically the excel of python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAMJeBTTKRWY"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "def import_data_from_drive(share_link, your_name_for_file=\"my_data\"):\n",
        "  \"\"\"Brings data file from a google drive sharepoint to your colab workspace.\n",
        "     It does not require you to host the dataset on your own account.\n",
        "\n",
        "     Parameters:\n",
        "     share_link: the link to view a file in google drive\n",
        "     our_name_for_file: a string describing the file, preferable endling in a file type, ex. 'data.csv'\n",
        "     \"\"\"\n",
        "  id = share_link.split(\"/\")[5] # separate the id from the link\n",
        "  print(\"Using id\", id, \"to find file on drive\")\n",
        "\n",
        "  # use pydrive and colab modules to authenticate you\n",
        "  auth.authenticate_user()\n",
        "  gauth = GoogleAuth()\n",
        "  gauth.credentials = GoogleCredentials.get_application_default()\n",
        "  drive = GoogleDrive(gauth)\n",
        "  print(\"Authenticated colab user\")\n",
        "\n",
        "  # This step will move the file from Drive to the workspace\n",
        "  download_object = drive.CreateFile({'id':id}) \n",
        "  download_object.GetContentFile(your_name_for_file)\n",
        "  print(\"Added file to workspace with name\", your_name_for_file)\n",
        "\n",
        "  return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgbHhB5lepKx"
      },
      "outputs": [],
      "source": [
        "import_data_from_drive(link, your_name_for_file=\"tweets.xlsx\")\n",
        "df = pd.read_excel('tweets.xlsx')\n",
        "# df = pd.read_csv('tweets.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzQPQ7XqvRgV"
      },
      "outputs": [],
      "source": [
        "# df is now an object, with associated methods we can use\n",
        "df.head(5) # lets look at the first five data samples\n",
        "# you can even access the spreadsheet in colab... "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-aTPgZyy2xh"
      },
      "outputs": [],
      "source": [
        "stop = stopwords.words('english')\n",
        "stop.extend([' ', 'ok', 'okay', 'via', 'this', 'that', 'it', 'lol', 'hah', 'haha', 'ha', 'like']) # you can add anythin to this ban list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udFNOIwYj1Mn"
      },
      "outputs": [],
      "source": [
        "# there is a problem with the data oh no!\n",
        "print(df['Full Text'][0])\n",
        "# Most language models probably don't know what the hell 'https://t.co/F5ak34HrCE' is"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ptyoYL7j1Mn"
      },
      "outputs": [],
      "source": [
        "# running this cell defines the function, does not run the function\n",
        "\n",
        "def clean(tweet):\n",
        "\n",
        "  # remove uppercase letters\n",
        "  tweet = tweet.lower()\n",
        "\n",
        "  # remove links\n",
        "  tweet = re.sub(r\"http\\S+\", \"\", tweet)\n",
        "\n",
        "  # remove everything but text\n",
        "  tweet = re.sub('^[A-Za-z0-9_]S+', \"\", tweet)\n",
        "\n",
        "\n",
        "  #some irregular punctuations need to be removed manually\n",
        "  tweet = re.sub(\"'|\\\"|’|…|”|“|’|…|’|“|”\",\"\" ,tweet)\n",
        "\n",
        "  #remove punctuations\n",
        "  temp = tweet.translate(str.maketrans('', '', string.punctuation))\n",
        "  tweet = \" \".join(temp.split())\n",
        "\n",
        "\n",
        "\n",
        "  return tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jfukKxKj1Mn"
      },
      "outputs": [],
      "source": [
        "df['Clean Text'] = df['Full Text'].apply(clean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qnn6lwauj1Mn"
      },
      "outputs": [],
      "source": [
        "print(df['Clean Text'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9jaMZ08wZCj"
      },
      "outputs": [],
      "source": [
        "model_path = \"bert-base-uncased\" #one language\n",
        "sentence_model = SentenceTransformer(model_path, device=\"cuda\")\n",
        "clusterer = hdbscan.HDBSCAN(min_cluster_size=30, min_samples=5, cluster_selection_method='leaf', prediction_data=True)\n",
        "# clusterer = KMeans(n_clusters=150)\n",
        "tokenizer = TweetTokenizer().tokenize\n",
        "vectorizer_model = CountVectorizer(ngram_range=(1, 1), stop_words=stop, tokenizer=tokenizer) # you could change \"ngrams\" to consider common word pairs or triplets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfhfzqkoSJ1I"
      },
      "outputs": [],
      "source": [
        "sentence_model = SentenceTransformer(model_path, device=\"cuda\")\n",
        "topic_model = BERTopic(embedding_model=sentence_model, top_n_words=10, calculate_probabilities=True, verbose=True, hdbscan_model = clusterer, vectorizer_model=vectorizer_model)\n",
        "topics, probs = topic_model.fit_transform(df['Clean Text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hfqQO1T0zEA"
      },
      "source": [
        "# Save to Excel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3lVFMHEA7Qi"
      },
      "outputs": [],
      "source": [
        "# cleaned tweets needs to be done different. \n",
        "\n",
        "tweet_by_topic = pd.DataFrame()\n",
        "tweet_df = df\n",
        "tweet_by_topic['topic'] = topic_model.topics_\n",
        "tweet_by_topic['probability'] = np.max(topic_model.probabilities_, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biSKXEaVzt3I"
      },
      "outputs": [],
      "source": [
        "tweet_by_topic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coAFOV_nBYmG"
      },
      "outputs": [],
      "source": [
        "topics = tweet_by_topic['topic'].copy()\n",
        "topics = pd.get_dummies(topics)\n",
        "topics.drop(columns=[0])\n",
        "topics['probability'] =  tweet_by_topic['probability'].copy()\n",
        "\n",
        "a = topics.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkQDcfsYCqMA"
      },
      "outputs": [],
      "source": [
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ol2K-nCuAe87"
      },
      "outputs": [],
      "source": [
        "\n",
        "def convert(row):\n",
        "  topics = row[1:-1]\n",
        "  prob = row[-1:]\n",
        "  # print(prob)\n",
        "\n",
        "\n",
        "  index = np.argmax(topics)\n",
        "  topics[index] = prob\n",
        "\n",
        "  return topics\n",
        "\n",
        "topics = topics.apply(convert, axis=1)\n",
        "topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cl6cMfwR09aE"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ig8tygHvFYl8"
      },
      "outputs": [],
      "source": [
        "tweet_by_topic = topics\n",
        "tweet_by_topic['text'] = tweet_df['Full Text'] \n",
        "tweet_by_topic['cleaned_text'] = tweet_df['Clean Text'] \n",
        "tweet_by_topic[\"Twitter Followers\"] =  tweet_df[\"Twitter Followers\"]\n",
        "tweet_by_topic[\"Twitter Reply Count\"] =  tweet_df[\"Twitter Reply Count\"]\n",
        "tweet_by_topic[\"Twitter Retweets\"] =  tweet_df[\"Twitter Retweets\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3eMQyhfGSrX"
      },
      "outputs": [],
      "source": [
        "tweet_by_topic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqmEzJj1m01d"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEnyD9i5hVQe"
      },
      "outputs": [],
      "source": [
        "tweet_by_topic.to_excel('tweet_by_topic-' + model_name +'with_sent.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNhQr_LCGc5B"
      },
      "outputs": [],
      "source": [
        "tweet_by_topic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeAT1SFHDEgF"
      },
      "outputs": [],
      "source": [
        "num_terms=10\n",
        "# array = [topic x word matrix, ndarray?]\n",
        "topic_by_words = pd.DataFrame(list(topic_model.topic_representations_.items())) # check axis\n",
        "a = pd.DataFrame(topic_by_words[1].to_list())\n",
        "c = pd.DataFrame(topic_by_words[0].to_list())\n",
        "topic_by_words = pd.DataFrame()\n",
        "series= pd.Series(topic_model.topic_sizes_.items())\n",
        "topic_by_words[\"topic\"] = series.apply(lambda x: x[0])\n",
        "topic_by_words[\"count\"] = series.apply(lambda x: x[1])\n",
        "for i in range(num_terms):\n",
        "  col = str(i + 1)\n",
        "  topic_by_words['term ' + col] = a[i].apply(lambda x: x[0])\n",
        "topic_by_words\n",
        "# topic_by_words['confirm topic'] = c\n",
        "\n",
        "#is\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vp_jMyUW0HmL"
      },
      "outputs": [],
      "source": [
        "topic_by_words.sort_values(by='topic', inplace=True)\n",
        "topic_by_words\n",
        "topic_by_words =topic_by_words.reset_index()\n",
        "topic_by_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANAgXCw1uOxm"
      },
      "outputs": [],
      "source": [
        "len(topic_model.get_representative_docs())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7l0S1jIZLOlk"
      },
      "outputs": [],
      "source": [
        "s_docs = pd.DataFrame(topic_model.representative_docs_.items())\n",
        "# s_docs = s_docs.sort_index()\n",
        "# print(docs[1][:][0])\n",
        "docs = pd.DataFrame()\n",
        "# b = pd.DataFrame(topic_by_words[1].to_list())\n",
        "docs['topic_#'] = s_docs[0]\n",
        "docs['sample 1'] = s_docs[1].apply(lambda x: x[0])\n",
        "docs['sample 2'] = s_docs[1].apply(lambda x: x[1])\n",
        "docs['sample 3'] = s_docs[1].apply(lambda x: x[2])\n",
        "ro = pd.DataFrame()\n",
        "ro['sample 1'] = [0]\n",
        "ro['sample 2'] = [0]\n",
        "ro['sample 3'] = [0]\n",
        "ro['topic_#'] = [-1]\n",
        "print(ro)\n",
        "docs = pd.concat((docs, ro), axis=0)\n",
        "\n",
        "docs = docs.sort_values(by=['topic_#'])\n",
        "docs = docs.reset_index()\n",
        "\n",
        "docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBDUHegUL8U1"
      },
      "outputs": [],
      "source": [
        "\n",
        "save_topics = pd.concat((topic_by_words, docs[['sample 1', 'sample 2', 'sample 3']]), axis=1)\n",
        "# save_topics = topic_by_words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2iO4B-vLKqIR"
      },
      "outputs": [],
      "source": [
        "save_topics = save_topics[1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTrT-UTsdAaV"
      },
      "outputs": [],
      "source": [
        "save_topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8xP8_D7M9CZ"
      },
      "outputs": [],
      "source": [
        "save_topics.to_excel(\"topics_info_\" + model_name +\".xlsx\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCBmyAi_15Qs"
      },
      "source": [
        "# Pretty Plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AnRu1bTy4Ri"
      },
      "source": [
        "### 2.1 Attributes\n",
        "\n",
        "There are a number of attributes that you can access after having trained your BERTopic model:\n",
        "\n",
        "\n",
        "| Attribute | Description |\n",
        "|------------------------|---------------------------------------------------------------------------------------------|\n",
        "| topics_               | The topics that are generated for each document after training or updating the topic model. |\n",
        "| probabilities_ | The probabilities that are generated for each document if HDBSCAN is used. |\n",
        "| topic_sizes_           | The size of each topic                                                                      |\n",
        "| topic_mapper_          | A class for tracking topics and their mappings anytime they are merged/reduced.             |\n",
        "| topic_representations_ | The top *n* terms per topic and their respective c-TF-IDF values.                             |\n",
        "| c_tf_idf_              | The topic-term matrix as calculated through c-TF-IDF.                                       |\n",
        "| topic_labels_          | The default labels for each topic.                                                          |\n",
        "| custom_labels_         | Custom labels for each topic as generated through `.set_topic_labels`.                                                               |\n",
        "| topic_embeddings_      | The embeddings for each topic if `embedding_model` was used.                                                              |\n",
        "| representative_docs_   | The representative documents for each topic if HDBSCAN is used.                                                |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4sEvkYfoPGy"
      },
      "outputs": [],
      "source": [
        "topics = topic_model.topics_\n",
        "probs = topic_model.probabilities_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0N4mDBPmbC4"
      },
      "outputs": [],
      "source": [
        "tweet_df.info()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yz-Py5ikzk9w"
      },
      "source": [
        "### 2.2 Visualizations\n",
        "\n",
        "More can be found [here](https://maartengr.github.io/BERTopic/getting_started/visualization/visualization.html#visualize-hierarchical-documents)\n",
        "\n",
        "| Method | Description |\n",
        "|------------------------|---------------------------------------------------------------------------------------------|\n",
        "|visualize_hierarchy              | In order to understand the potential hierarchical structure of the topics, we can use scipy.cluster.hierarchy to create clusters and visualize how they relate to one another.|\n",
        "| visualize_topics    | We embed our representation of the topics in 2D using Umap and then create an interactive view|\n",
        "| visualize_barchart         | We can visualize the selected terms for a few topics by creating bar charts out of the c-TF-IDF scores for each topic representation|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5a5-tWeV3c2"
      },
      "outputs": [],
      "source": [
        "topic_model.visualize_hierarchy(top_n_topics = 70, custom_labels=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oLloTbxGvmE"
      },
      "outputs": [],
      "source": [
        "topic_model.visualize_topics(top_n_topics=60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRJIvIkry-VE"
      },
      "outputs": [],
      "source": [
        "topic_model.visualize_topics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZzGT5BDxHcJ"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model_path = \"cardiffnlp/twitter-roberta-base\" #one language\n",
        "sentence_model = SentenceTransformer(model_path, device=\"cuda\")\n",
        "# sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yh4B5mGuxD0Q"
      },
      "outputs": [],
      "source": [
        "from umap import UMAP\n",
        "embeddings = sentence_model.encode(df['Clean Text'], show_progress_bar=True)\n",
        "# umap_model = UMAP(n_neighbors=10, n_components=5, min_dist=0.1, metric='cosine', random_state=42)\n",
        "reduced_embeddings = UMAP(n_neighbors=15, n_components=12, \n",
        "                          min_dist=0.0, metric='cosine').fit_transform(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YPn8DTix2If"
      },
      "outputs": [],
      "source": [
        "topic_model.visualize_documents(df['Clean Text'], reduced_embeddings=reduced_embeddings,\n",
        "                                hide_document_hover=False, hide_annotations=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjnxJvhi2AS2"
      },
      "outputs": [],
      "source": [
        "topic_model.visualize_barchart(top_n_topics=20, n_words=5) # not the best tool in my opinion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgVr_qFT3d07"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMsTbL5xMoG/2f1ZV/G+hdN",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}